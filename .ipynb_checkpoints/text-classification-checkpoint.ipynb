{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simples classificação de texto usando Deep Learning com Python\n",
    "\n",
    "Repositório Github (https://github.com/Richard-Teske/text-classification-keras) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando arquivos de treino / teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import zipfile\n",
    "\n",
    "begin = datetime.datetime.now()\n",
    "\n",
    "file_path = %pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\GitHub\\\\text-classification-stack'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extração dos arquivos de treino e teste zipados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = 'dataset'\n",
    "zip_file = 'dataset.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'train.csv'\n",
    "test_file = 'test.csv'\n",
    "\n",
    "train_path = os.path.join(file_path, dataset_folder, train_file)\n",
    "test_path = os.path.join(file_path, dataset_folder, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(os.path.join(file_path,dataset_folder,train_file)):\n",
    "    with zipfile.ZipFile(os.path.join(file_path, dataset_folder,zip_file), 'r') as zip:\n",
    "        zip.extractall(os.path.join(file_path, dataset_folder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\GitHub\\text-classification-stack\\dataset\\train.csv\n",
      "D:\\GitHub\\text-classification-stack\\dataset\\test.csv\n"
     ]
    }
   ],
   "source": [
    "print(train_path)\n",
    "print(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>*Tenha certeza que este é o diretório certo dos arquivos de treino e teste no seu ambiente*</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_original = pd.read_csv(train_path, sep=';', header=None, names=['Body','Tag','Title'])\n",
    "df_test_original = pd.read_csv(test_path, sep=';', header=None, names=['Body','Tag','Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;p&gt;I am new to Silverlight 2.0 and I am actual...</td>\n",
       "      <td>silverlight-2.0</td>\n",
       "      <td>Silverlight WebPart in Sharepoint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;p&gt;I have been used to do some refactorings by...</td>\n",
       "      <td>refactoring</td>\n",
       "      <td>Is Refactoring by Compilation Errors Bad?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;p&gt;I've seen this questions &lt;a href=\"https://s...</td>\n",
       "      <td>coding-style</td>\n",
       "      <td>What is the name of this particular indent sty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;p&gt;How do you like your CRUD programs.  Code-g...</td>\n",
       "      <td>refactoring</td>\n",
       "      <td>Code Generator vs Code Refactoring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;p&gt;Reading &lt;a href=\"https://stackoverflow.com/...</td>\n",
       "      <td>build-process</td>\n",
       "      <td>Continuous Integration vs. Nightly Builds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;p&gt;It should first be noted that I am trying t...</td>\n",
       "      <td>build-process</td>\n",
       "      <td>VBC + NAnt. Error compiling WinForm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;p&gt;SiteA.com and siteB.com are .NET 2.0 apps o...</td>\n",
       "      <td>.net-2.0</td>\n",
       "      <td>How can one IIS6 .NET app appear as subsite of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;p&gt;In .NET 1.x, you could use the &lt;a href=\"htt...</td>\n",
       "      <td>.net-2.0</td>\n",
       "      <td>How can I prevent unauthorized code from acces...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;p&gt;I heard google has some automated process l...</td>\n",
       "      <td>build-process</td>\n",
       "      <td>Build Server Best Practices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;p&gt;I am trying to run a .Net 2.0 application f...</td>\n",
       "      <td>.net-2.0</td>\n",
       "      <td>.Net 2.0 application from network share withou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Body              Tag  \\\n",
       "0  <p>I am new to Silverlight 2.0 and I am actual...  silverlight-2.0   \n",
       "1  <p>I have been used to do some refactorings by...      refactoring   \n",
       "2  <p>I've seen this questions <a href=\"https://s...     coding-style   \n",
       "3  <p>How do you like your CRUD programs.  Code-g...      refactoring   \n",
       "4  <p>Reading <a href=\"https://stackoverflow.com/...    build-process   \n",
       "5  <p>It should first be noted that I am trying t...    build-process   \n",
       "6  <p>SiteA.com and siteB.com are .NET 2.0 apps o...         .net-2.0   \n",
       "7  <p>In .NET 1.x, you could use the <a href=\"htt...         .net-2.0   \n",
       "8  <p>I heard google has some automated process l...    build-process   \n",
       "9  <p>I am trying to run a .Net 2.0 application f...         .net-2.0   \n",
       "\n",
       "                                               Title  \n",
       "0                  Silverlight WebPart in Sharepoint  \n",
       "1          Is Refactoring by Compilation Errors Bad?  \n",
       "2  What is the name of this particular indent sty...  \n",
       "3                 Code Generator vs Code Refactoring  \n",
       "4          Continuous Integration vs. Nightly Builds  \n",
       "5                VBC + NAnt. Error compiling WinForm  \n",
       "6  How can one IIS6 .NET app appear as subsite of...  \n",
       "7  How can I prevent unauthorized code from acces...  \n",
       "8                        Build Server Best Practices  \n",
       "9  .Net 2.0 application from network share withou...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_original.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamento de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `Este dataset provem do banco de dados publico do StackOverflow, que você poderá baixa-lo gratuitamente na internet o seu arquivo MDF, na estrutura original dos dados, existem algumas tags HTML no Body que precisam ser tratadas, além de algumas stopwords que precisam ser retiradas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "\n",
    "stopWords_original = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Existe uma lista de stopwords já definidas pelo NLTK em diferentes linguagens (também em Português).\n",
    "Nas stopwords originais eu optei por trazer o tratamento de dados nela, retirando os caracteres especiais e deixando todas em lower case (Que já estão em sua lista original)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopWords_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " 'youre',\n",
       " 'youve',\n",
       " 'youll',\n",
       " 'youd',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " 'shes',\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " 'its',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " 'thatll',\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " 'dont',\n",
       " 'should',\n",
       " 'shouldve',\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " 'arent',\n",
       " 'couldn',\n",
       " 'couldnt',\n",
       " 'didn',\n",
       " 'didnt',\n",
       " 'doesn',\n",
       " 'doesnt',\n",
       " 'hadn',\n",
       " 'hadnt',\n",
       " 'hasn',\n",
       " 'hasnt',\n",
       " 'haven',\n",
       " 'havent',\n",
       " 'isn',\n",
       " 'isnt',\n",
       " 'ma',\n",
       " 'mightn',\n",
       " 'mightnt',\n",
       " 'mustn',\n",
       " 'mustnt',\n",
       " 'needn',\n",
       " 'neednt',\n",
       " 'shan',\n",
       " 'shant',\n",
       " 'shouldn',\n",
       " 'shouldnt',\n",
       " 'wasn',\n",
       " 'wasnt',\n",
       " 'weren',\n",
       " 'werent',\n",
       " 'won',\n",
       " 'wont',\n",
       " 'wouldn',\n",
       " 'wouldnt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopWords = []\n",
    "\n",
    "for word in stopWords_original:\n",
    "    stopWords.append(re.sub('[^a-z]+','',word.lower()))\n",
    "\n",
    "stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame()\n",
    "df_train = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Retira todas os caracteres especiais dos arquivos de treino e teste (Inclusive tags HTML do Body) e os deixa em Lower Case.\n",
    "Também é feita a unificação do Body e o Title do dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>**Test**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, rows in df_test_original.iterrows():\n",
    "\n",
    "    title = word_tokenize(re.sub('[^a-z]+',' ',rows['Title'].lower()))\n",
    "    body = word_tokenize(re.sub('[^a-z]+',' ',re.sub('<[^>]+>',' ',rows['Body'].lower())))\n",
    "\n",
    "    content = []\n",
    "    content.append([w for w in title + body if w not in stopWords])\n",
    "\n",
    "    data = []\n",
    "    data.append((' '.join(content[0]), rows['Tag']))\n",
    "\n",
    "    df_test = pd.concat([df_test, pd.DataFrame(data)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>**Train**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, rows in df_train_original.iterrows():\n",
    "\n",
    "    title = word_tokenize(re.sub('[^a-z]+',' ',rows['Title'].lower()))\n",
    "    body = word_tokenize(re.sub('[^a-z]+',' ',re.sub('<[^>]+>',' ',rows['Body'].lower())))\n",
    "\n",
    "    content = []\n",
    "    content.append([w for w in title + body if w not in stopWords])\n",
    "\n",
    "    data = []\n",
    "    data.append((' '.join(content[0]), rows['Tag']))\n",
    "\n",
    "    df_train = pd.concat([df_train, pd.DataFrame(data)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.columns = ['Content','Tag']\n",
    "df_train.columns = ['Content', 'Tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>determine user timezone standard way web serve...</td>\n",
       "      <td>javascript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>swap unique indexed column values database dat...</td>\n",
       "      <td>sql</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>efficient code first prime numbers want print ...</td>\n",
       "      <td>performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>build windows nt using visual studio mfc appli...</td>\n",
       "      <td>c++</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>best way access exchange using php writing cms...</td>\n",
       "      <td>php</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deploying sql server databases test live wonde...</td>\n",
       "      <td>sql-server</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>programmatically launch ie mobile favorites sc...</td>\n",
       "      <td>internet-explorer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lucene score results lucene multiple indexes c...</td>\n",
       "      <td>search</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>use unsigned values signed ones appropriate us...</td>\n",
       "      <td>language-agnostic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>use nested classes case working collection cla...</td>\n",
       "      <td>c++</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content                Tag\n",
       "0  determine user timezone standard way web serve...         javascript\n",
       "1  swap unique indexed column values database dat...                sql\n",
       "2  efficient code first prime numbers want print ...        performance\n",
       "3  build windows nt using visual studio mfc appli...                c++\n",
       "4  best way access exchange using php writing cms...                php\n",
       "5  deploying sql server databases test live wonde...         sql-server\n",
       "6  programmatically launch ie mobile favorites sc...  internet-explorer\n",
       "7  lucene score results lucene multiple indexes c...             search\n",
       "8  use unsigned values signed ones appropriate us...  language-agnostic\n",
       "9  use nested classes case working collection cla...                c++"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processamento de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras import utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_Words = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Numero de palavras maxima para o modelo de treinamento (top N palavras)\n",
    "**OBS: Caso o valor seja aumentado, irá aumentar também o consumo de memoria da sua maquina\n",
    "Fique ligado a quanto de memoria você poderá utilizar no treinamento, caso contrato irá ocorrer erro de memoria`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=num_Words)\n",
    "tokenizer.fit_on_texts(df_train['Content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformação de textos para matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tokenizer.texts_to_matrix(df_train['Content'])\n",
    "x_test = tokenizer.texts_to_matrix(df_test['Content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(df_train['Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = encoder.transform(df_train['Tag'])\n",
    "y_test = encoder.transform(df_test['Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = np.max(y_test) + 1\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = utils.to_categorical(y_train, num_classes=n_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Numero de exemplos a serem treinados por vez\n",
    "batch_size = 100\n",
    "## Numero de vezes que a rede neural irá treinar todo o dataset\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(num_Words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(  loss='categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 209956 samples, validate on 23329 samples\n",
      "Epoch 1/10\n",
      " - 30s - loss: 1.1121 - acc: 0.7297 - val_loss: 0.8776 - val_acc: 0.7691\n",
      "Epoch 2/10\n",
      " - 29s - loss: 0.8244 - acc: 0.7692 - val_loss: 0.8286 - val_acc: 0.7746\n",
      "Epoch 3/10\n",
      " - 30s - loss: 0.7505 - acc: 0.7821 - val_loss: 0.8233 - val_acc: 0.7768\n",
      "Epoch 4/10\n",
      " - 30s - loss: 0.6997 - acc: 0.7921 - val_loss: 0.8226 - val_acc: 0.7763\n",
      "Epoch 5/10\n",
      " - 31s - loss: 0.6543 - acc: 0.8022 - val_loss: 0.8260 - val_acc: 0.7746\n",
      "Epoch 6/10\n",
      " - 31s - loss: 0.6159 - acc: 0.8114 - val_loss: 0.8393 - val_acc: 0.7744\n",
      "Epoch 7/10\n",
      " - 31s - loss: 0.5809 - acc: 0.8197 - val_loss: 0.8578 - val_acc: 0.7762\n",
      "Epoch 8/10\n",
      " - 32s - loss: 0.5511 - acc: 0.8257 - val_loss: 0.8724 - val_acc: 0.7758\n",
      "Epoch 9/10\n",
      " - 33s - loss: 0.5275 - acc: 0.8327 - val_loss: 0.9057 - val_acc: 0.7728\n",
      "Epoch 10/10\n",
      " - 32s - loss: 0.5007 - acc: 0.8395 - val_loss: 0.9222 - val_acc: 0.7714\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=2,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56900/56900 [==============================] - 2s 28us/step\n",
      "Test score:  0.8788562448718217\n",
      "Test accuracy:  0.7733567661895484\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test,\n",
    "                    batch_size= batch_size,\n",
    "                    verbose=1)\n",
    "\n",
    "print('Test score: ', score[0])\n",
    "print('Test accuracy: ',score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Todas as Tags usadas no treinamento`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['.net-2.0', '.net-3.5', 'actionscript', 'actionscript-3',\n",
       "       'activerecord', 'ado.net', 'air', 'amazon-web-services',\n",
       "       'android-emulator', 'ant', 'apache', 'api', 'arrays',\n",
       "       'asp-classic', 'automated-tests', 'azure', 'bash', 'build-process',\n",
       "       'button', 'c++', 'caching', 'cassandra', 'charts', 'cmake',\n",
       "       'cocos2d-iphone', 'coding-style', 'collections',\n",
       "       'compiler-construction', 'cookies', 'core-data', 'cryptography',\n",
       "       'crystal-reports', 'csv', 'database-design', 'delphi',\n",
       "       'dependency-injection', 'design-patterns', 'devexpress',\n",
       "       'dictionary', 'diff', 'django', 'dll', 'dynamic', 'dynamics-crm',\n",
       "       'embedded', 'ffmpeg', 'file-io', 'file-upload', 'filesystems',\n",
       "       'firefox', 'floating-point', 'fortran', 'frameworks', 'function',\n",
       "       'g++', 'geometry', 'google-app-engine', 'graph', 'html5', 'iframe',\n",
       "       'image', 'indexing', 'installer', 'internet-explorer', 'ios4',\n",
       "       'javascript', 'jpa', 'jqgrid', 'jquery-plugins', 'json', 'jsp',\n",
       "       'junit', 'language-agnostic', 'licensing', 'linq-to-sql', 'lisp',\n",
       "       'logging', 'loops', 'lucene', 'matlab', 'maven', 'mediawiki',\n",
       "       'memory-management', 'merge', 'module', 'ms-word', 'msbuild',\n",
       "       'multithreading', 'nhibernate', 'objective-c', 'operating-system',\n",
       "       'optimization', 'performance', 'php', 'programming-languages',\n",
       "       'recursion', 'refactoring', 'resharper', 'resources', 'rest',\n",
       "       'ruby-on-rails-3', 'search', 'sed', 'sharepoint-2010',\n",
       "       'silverlight-2.0', 'sms', 'sockets', 'spring-security', 'sql',\n",
       "       'sql-server', 'sql-server-2000', 'sql-server-2005', 'sqlalchemy',\n",
       "       'svn', 'syntax', 'textmate', 'uitableview', 'unicode', 'video',\n",
       "       'vim', 'visual-studio', 'web-applications', 'winapi', 'windows-ce',\n",
       "       'windows-services', 'wordpress', 'wpf', 'x86', 'xaml', 'xml',\n",
       "       'xna', 'xsd', 'xslt', 'xul', 'youtube', 'zend-framework'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = encoder.classes_\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Testes com alguns dados do dataset original`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pergunta: Determine a User's Timezone\n",
      "Label atual:javascript\n",
      "Label preditivo: php\n",
      "\n",
      "Pergunta: Swap unique indexed column values in database\n",
      "Label atual:sql\n",
      "Label preditivo: sql\n",
      "\n",
      "Pergunta: Most efficient code for the first 10000 prime numbers?\n",
      "Label atual:performance\n",
      "Label preditivo: c++\n",
      "\n",
      "Pergunta: Build for Windows NT 4.0 using Visual Studio 2005?\n",
      "Label atual:c++\n",
      "Label preditivo: c++\n",
      "\n",
      "Pergunta: Best way to access Exchange using PHP?\n",
      "Label atual:php\n",
      "Label preditivo: php\n",
      "\n",
      "Pergunta: Deploying SQL Server Databases from Test to Live\n",
      "Label atual:sql-server\n",
      "Label preditivo: sql\n",
      "\n",
      "Pergunta: Programmatically launch IE Mobile favorites screen\n",
      "Label atual:internet-explorer\n",
      "Label preditivo: internet-explorer\n",
      "\n",
      "Pergunta: Lucene Score results\n",
      "Label atual:search\n",
      "Label preditivo: search\n",
      "\n",
      "Pergunta: When to use unsigned values over signed ones?\n",
      "Label atual:language-agnostic\n",
      "Label preditivo: loops\n",
      "\n",
      "Pergunta: Should I use nested classes in this case?\n",
      "Label atual:c++\n",
      "Label preditivo: c++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(0,10):\n",
    "    prediction = model.predict(np.array([x_test[n]]))\n",
    "    predicted_label = labels[np.argmax(prediction)]\n",
    "    print('Pergunta: '+ df_test_original['Title'].iloc[n])\n",
    "    print('Label atual:' + df_test_original['Tag'].iloc[n])\n",
    "    print(\"Label preditivo: \" + predicted_label + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo estimado de execução: 1 horas e 62 minutos\n"
     ]
    }
   ],
   "source": [
    "end = datetime.datetime.now()\n",
    "\n",
    "diff = end - begin\n",
    "\n",
    "print('Tempo estimado de execução: %d horas e %d minutos'%(diff.seconds/3600, diff.seconds/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*`Você pode salvar o modelo gerado em um arquivo para utilizar em uma classificação sem precisar re-treinar`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model.save('model.h5')\n",
    "np.save('labels.npy',labels)\n",
    "with open('tokenizer.pickle','wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "` É importante tambem salvar seus labels e o tokenizer que contem algumas propriedades usadas no treinamento\n",
    "O label é salvo em um arquivo .npy (numpy), o tokenizer em um arquivo .pickle (pickle) e o modelo em um arquivo .h5 (HDF5)\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=red>IMPORTANTE: Caso grave o arquivo labels ou tokenizer em outro formato, tenha certeza que os arquivos estejam com suas propriedades na forma original em que foram treinadas.\n",
    "Qualquer ordem que não estejam conforme os treinamentos iram retornar resultados errados na predição</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Para re-utilizar os arquivos do treinamento, apenas carregue eles para o modelo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('model.h5')\n",
    "labels = np.load('labels.npy')\n",
    "with open('tokenizer.pickle','rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['How can I redirect the user from one page to another using jQuery or pure JavaScript?',\n",
    "        'What is the difference between “INNER JOIN” and “OUTER JOIN” in SQL Server?',\n",
    "        'How do you give a C# Auto-Property a default value? I either use the constructor, or revert to the old syntax.',\n",
    "        'In PHP 5, what is the difference between using self and $this? When is each appropriate?',\n",
    "        'What are valid values for the id attribute in HTML?',\n",
    "        'Optimizing Lucene performance',\n",
    "        'Continuous Integration System for Delphi',\n",
    "        'How does database indexing work?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pergunta:  How can I redirect the user from one page to another using jQuery or pure JavaScript?\n",
      "Tag preditiva:  javascript\n",
      "\n",
      "\n",
      "Pergunta:  What is the difference between “INNER JOIN” and “OUTER JOIN” in SQL Server?\n",
      "Tag preditiva:  sql-server\n",
      "\n",
      "\n",
      "Pergunta:  How do you give a C# Auto-Property a default value? I either use the constructor, or revert to the old syntax.\n",
      "Tag preditiva:  c++\n",
      "\n",
      "\n",
      "Pergunta:  In PHP 5, what is the difference between using self and $this? When is each appropriate?\n",
      "Tag preditiva:  php\n",
      "\n",
      "\n",
      "Pergunta:  What are valid values for the id attribute in HTML?\n",
      "Tag preditiva:  html5\n",
      "\n",
      "\n",
      "Pergunta:  Optimizing Lucene performance\n",
      "Tag preditiva:  performance\n",
      "\n",
      "\n",
      "Pergunta:  Continuous Integration System for Delphi\n",
      "Tag preditiva:  delphi\n",
      "\n",
      "\n",
      "Pergunta:  How does database indexing work?\n",
      "Tag preditiva:  sql\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in texts:\n",
    "    data = []\n",
    "    data.append(t)\n",
    "    X = tokenizer.texts_to_matrix(data)\n",
    "    prediction = model.predict(X)\n",
    "    predicted_label = labels[np.argmax(prediction)]\n",
    "    print('Pergunta: ',t)\n",
    "    print('Tag preditiva: ',predicted_label)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Isso é tudo, sugestões e criticas serão bem vindas, bons estudos e obrigado por comparecer.</center>\n",
    "#### <center>Qualquer duvida estou a disposição :)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinkedIn (https://www.linkedin.com/in/richard-teske-25b88214b/) <br>\n",
    "Email (richardaraujo.dba@gmail.com) <br>\n",
    "GitHub (https://github.com/Richard-Teske)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
